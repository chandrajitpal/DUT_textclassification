{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSJwBRaqQrhX0DKOfj+z2R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chandrajitpal/DUT_textclassification/blob/main/DUT_binarisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJEhpWuMP_dn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Step 2: Load and Prepare the Data\n",
        "try:\n",
        "    # Load the dataset from the CSV file\n",
        "    file_path = 'Train_Binary_22July2025.xlsx - Binary classification.csv'\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    print(\"Dataset loaded successfully. Here are the first 5 rows:\")\n",
        "    print(df.head())\n",
        "    print(\"\\nDataset Information:\")\n",
        "    df.info()\n",
        "\n",
        "    # --- Data Cleaning and Preprocessing ---\n",
        "    # We only need the 'Abstract' and 'Yes/No' columns.\n",
        "    # Let's drop rows where either of these is missing.\n",
        "    df_clean = df[['Abstract', 'Yes/No']].dropna()\n",
        "\n",
        "    # Convert the target labels 'Yes'/'No' to numerical format (1/0)\n",
        "    # This is required for most machine learning models.\n",
        "    df_clean['label'] = df_clean['Yes/No'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "    # Define our features (X) and target (y)\n",
        "    X = df_clean['Abstract']\n",
        "    y = df_clean['label']\n",
        "\n",
        "    print(f\"\\nNumber of samples after cleaning: {len(df_clean)}\")\n",
        "    print(f\"Class distribution:\\n{df_clean['Yes/No'].value_counts()}\")\n",
        "\n",
        "    # Step 3: Split the data into training and testing sets\n",
        "    # 80% for training, 20% for testing.\n",
        "    # random_state ensures we get the same split every time we run the script, for reproducibility.\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTraining set size: {len(X_train)}\")\n",
        "    print(f\"Testing set size: {len(X_test)}\")\n",
        "\n",
        "    # Step 4: Build the AI Model Pipeline\n",
        "    # A pipeline chains together multiple steps. Here, it will:\n",
        "    # 1. Convert text to TF-IDF vectors.\n",
        "    # 2. Train an SGD Classifier on these vectors.\n",
        "    model_pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer(stop_words='english', ngram_range=(1, 2))),\n",
        "        ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=50, tol=None)),\n",
        "    ])\n",
        "\n",
        "    # Step 5: Train the AI Model\n",
        "    print(\"\\nTraining the model...\")\n",
        "    model_pipeline.fit(X_train, y_train)\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "    # Step 6: Evaluate the Model\n",
        "    print(\"\\nEvaluating model performance on the test set...\")\n",
        "    y_pred = model_pipeline.predict(X_test)\n",
        "\n",
        "    # Calculate and print the accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"\\nModel Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "    # Print a detailed classification report (precision, recall, f1-score)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    # Use target_names to show 'Yes' and 'No' instead of 1 and 0\n",
        "    print(classification_report(y_test, y_pred, target_names=['No', 'Yes']))\n",
        "\n",
        "    # Step 7: Create a Prediction Function\n",
        "    def predict_abstract(abstract_text):\n",
        "        \"\"\"\n",
        "        Takes a new abstract text as input and predicts if it's 'Yes' or 'No'.\n",
        "        \"\"\"\n",
        "        # The model pipeline handles both vectorization and prediction\n",
        "        prediction = model_pipeline.predict([abstract_text])\n",
        "\n",
        "        # Convert the numerical prediction back to a human-readable label\n",
        "        return 'Yes' if prediction[0] == 1 else 'No'\n",
        "\n",
        "    # --- Example Usage of the Predictor ---\n",
        "    print(\"\\n--- Testing the Predictor with New Examples ---\")\n",
        "\n",
        "    # Example 1 (likely 'Yes' based on keywords like 'well-being' and 'climate change')\n",
        "    abstract1 = \"This study examines the direct impact of climate change on the socio-economic well-being of coastal communities. We analyze how rising sea levels affect health and livelihood.\"\n",
        "    print(f\"\\nAbstract: '{abstract1}'\")\n",
        "    print(f\"Prediction: {predict_abstract(abstract1)}\")\n",
        "\n",
        "    # Example 2 (likely 'No' as it's not directly about climate change and well-being)\n",
        "    abstract2 = \"We present a new algorithm for data compression using neural networks. The focus is on computational efficiency and lossless compression ratios for large datasets.\"\n",
        "    print(f\"\\nAbstract: '{abstract2}'\")\n",
        "    print(f\"Prediction: {predict_abstract(abstract2)}\")\n",
        "\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file 'Train_Binary_22July2025.xlsx - Binary classification.csv' was not found.\")\n",
        "    print(\"Please make sure the CSV file is in the same directory as this script.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n"
      ]
    }
  ]
}